{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25654769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"--- [Step 1] EDA Script Started ---\")\n",
    "\n",
    "# Graph-Defaults set karna\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 7)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- 1. Data Loading ---\n",
    "try:\n",
    "    df = pd.read_csv('jhajjar_data_60_finalNEW.csv')\n",
    "    print(\"CSV file 'jhajjar_data_60_finalNEW.csv' successfully loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'jhajjar_data_60_finalNEW.csv' file nahi mili.\")\n",
    "    sys.exit()\n",
    "except Exception as e:\n",
    "    print(f\"File load karte waqt error: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 2. Data Preprocessing (EDA ke liye) ---\n",
    "print(\"\\n--- Data Preprocessing ---\")\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df.set_index('time', inplace=True)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.ffill(inplace=True)\n",
    "df.bfill(inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "# --- 3. Exploratory Data Analysis (Graphs Dikhana) ---\n",
    "print(\"\\n--- Exploratory Data Analysis (Showing Graphs) ---\")\n",
    "\n",
    "if 'pm2_5' not in df.columns:\n",
    "    print(\"Error: 'pm2_5' column zaroori hai EDA ke liye.\")\n",
    "    sys.exit()\n",
    "\n",
    "# EDA 1: PM2.5 over Time\n",
    "print(\"Showing Graph 1: PM2.5 over Time...\")\n",
    "df['pm2_5'].plot(title='PM2.5 Levels Over Time', color='blue', alpha=0.7)\n",
    "plt.ylabel('PM2.5 (µg/m³)')\n",
    "plt.xlabel('Date')\n",
    "plt.show() # <-- Badlaav: Ab graph show hoga\n",
    "\n",
    "# EDA 2: PM2.5 Distribution (Histogram)\n",
    "print(\"Showing Graph 2: Distribution of PM2.5...\")\n",
    "sns.histplot(df['pm2_5'], kde=True, bins=50)\n",
    "plt.title('Distribution of PM2.5 Values')\n",
    "plt.xlabel('PM2.5 (µg/m³)')\n",
    "plt.ylabel('Frequency (Count)')\n",
    "plt.show() # <-- Badlaav: Ab graph show hoga\n",
    "\n",
    "# EDA 3: Time-based features (Analysis ke liye)\n",
    "df['month'] = df.index.month\n",
    "df['hour'] = df.index.hour\n",
    "\n",
    "# EDA 4: Seasonal Analysis (PM2.5 by Month)\n",
    "print(\"Showing Graph 3: PM2.5 by Month...\")\n",
    "sns.boxplot(x='month', y='pm2_5', data=df)\n",
    "plt.title('Monthly PM2.5 Distribution (Seasonal Trend)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('PM2.5 (µg/m³)')\n",
    "plt.show() # <-- Badlaav: Ab graph show hoga\n",
    "\n",
    "# EDA 5: Diurnal Analysis (PM2.5 by Hour)\n",
    "print(\"Showing Graph 4: PM2.5 by Hour...\")\n",
    "sns.boxplot(x='hour', y='pm2_5', data=df)\n",
    "plt.title('Hourly PM2.5 Distribution (Diurnal Trend)')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('PM2.5 (µg/m³)')\n",
    "plt.show() # <-- Badlaav: Ab graph show hoga\n",
    "\n",
    "# --- 4. Correlation Analysis ---\n",
    "print(\"\\n--- Correlation Analysis ---\")\n",
    "\n",
    "# Features list (Correlation ke liye)\n",
    "selected_features = [\n",
    "    'temperature_2m', 'relativehumidity_2m', 'dewpoint_2m', 'apparent_temperature',\n",
    "    'pressure_msl', 'surface_pressure', 'vapor_pressure_deficit', 'precipitation',\n",
    "    'cloudcover', 'shortwave_radiation', 'direct_radiation', 'diffuse_radiation',\n",
    "    'windspeed_10m', 'winddirection_10m', 'windgusts_10m', 'et0_fao_evapotranspiration',\n",
    "    'soil_temperature_0cm', 'soil_moisture_0_1cm', 'visibility', 'cape',\n",
    "    'month', 'hour', 'pm10', 'pm2_5', 'carbon_monoxide', 'nitrogen_dioxide',\n",
    "    'sulphur_dioxide', 'ozone', 'dust', 'ammonia'\n",
    "]\n",
    "\n",
    "available_features = [col for col in selected_features if col in df.columns]\n",
    "if len(available_features) != len(selected_features):\n",
    "    print(f\"Missing columns: {set(selected_features) - set(available_features)}\")\n",
    "\n",
    "corr_matrix = df[available_features].corr()\n",
    "target_corr = corr_matrix['pm2_5'].sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 features most correlated with PM2.5:\")\n",
    "print(target_corr.head(11))\n",
    "\n",
    "print(\"Showing Graph 5: Correlation Heatmap...\")\n",
    "top_features = target_corr.index[:20]\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df[top_features].corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap of Top 20 Features (vs PM2.5)')\n",
    "plt.tight_layout()\n",
    "plt.show() # <-- Badlaav: Ab graph show hoga\n",
    "\n",
    "print(\"\\n--- [Step 1] EDA Script Finished ---\")\n",
    "print(\"Saare graphs dikha diye gaye hain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a4aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"--- [Step 2] PCA Analysis Script Started ---\")\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- 1. Data Loading ---\n",
    "try:\n",
    "    df = pd.read_csv('jhajjar_data_60_finalNEW.csv')\n",
    "    print(\"CSV file successfully loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'jhajjar_data_60_finalNEW.csv' file nahi mili.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 2. Data Preprocessing ---\n",
    "print(\"\\n--- Data Preprocessing ---\")\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df.set_index('time', inplace=True)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.ffill(inplace=True); df.bfill(inplace=True); df.fillna(0, inplace=True)\n",
    "df['month'] = df.index.month\n",
    "df['day_of_week'] = df.index.dayofweek\n",
    "df['hour'] = df.index.hour\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "# --- 3. Feature Selection ---\n",
    "# Features (X) ko define karna\n",
    "selected_features = [\n",
    "    'temperature_2m', 'relativehumidity_2m', 'dewpoint_2m', 'apparent_temperature',\n",
    "    'pressure_msl', 'surface_pressure', 'vapor_pressure_deficit', 'precipitation',\n",
    "    'cloudcover', 'shortwave_radiation', 'direct_radiation', 'diffuse_radiation',\n",
    "    'windspeed_10m', 'winddirection_10m', 'windgusts_10m', 'et0_fao_evapotranspiration',\n",
    "    'soil_temperature_0cm', 'soil_moisture_0_1cm', 'visibility', 'cape',\n",
    "    'month', 'day_of_week', 'hour',\n",
    "    'pm10', 'pm2_5', 'carbon_monoxide', 'nitrogen_dioxide', 'sulphur_dioxide',\n",
    "    'ozone', 'dust', 'ammonia'\n",
    "]\n",
    "missing_cols = [col for col in selected_features if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"!!! Error: Missing features: {missing_cols}\")\n",
    "    selected_features = [col for col in selected_features if col not in missing_cols]\n",
    "\n",
    "X = df[selected_features]\n",
    "\n",
    "# --- 4. Data Scaling (PCA ke liye ZAROORI) ---\n",
    "print(\"\\n--- Data Scaling ---\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"Data scaling complete.\")\n",
    "\n",
    "# --- 5. PCA (Principal Component Analysis) ---\n",
    "print(\"\\n--- [Step 2] Running PCA ---\")\n",
    "# Hum 95% variance ko capture karna chahte hain\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"\\nOriginal number of features: {X_scaled.shape[1]}\")\n",
    "print(f\"Features after PCA (for 95% variance): {pca.n_components_}\")\n",
    "\n",
    "# Explained Variance Plot\n",
    "print(\"\\nShowing Graph: PCA Explained Variance...\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA Explained Variance Plot')\n",
    "plt.grid(True)\n",
    "plt.show() # <-- Badlaav: Ab graph show hoga\n",
    "\n",
    "print(\"\\n--- Top 5 PCA Components Analysis ---\")\n",
    "print(\"Yeh batata hai ki har component kin features se bana hai:\")\n",
    "for i in range(min(5, pca.n_components_)): # Top 5 ya jitne bane hain\n",
    "    component = pca.components_[i]\n",
    "    # Get top 5 features in this component\n",
    "    top_5_indices = np.argsort(np.abs(component))[-5:][::-1]\n",
    "    top_5_features = [selected_features[j] for j in top_5_indices]\n",
    "    print(f\"  Component {i+1} is most driven by: {top_5_features}\")\n",
    "\n",
    "print(\"\\n--- [Step 2] PCA Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09226fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib # Model save karne ke liye\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"--- [Step 3] Naive Bayes Classification (TRAIN) Script Started ---\")\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- 1. Data Loading & Preprocessing ---\n",
    "try:\n",
    "    df_full = pd.read_csv('jhajjar_data_60_finalNEW.csv')\n",
    "    print(\"CSV file successfully loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'jhajjar_data_60_finalNEW.csv' file nahi mili.\")\n",
    "    sys.exit()\n",
    "\n",
    "print(\"\\n--- Data Preprocessing ---\")\n",
    "df_full['time'] = pd.to_datetime(df_full['time'])\n",
    "df_full.set_index('time', inplace=True)\n",
    "df_full.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_full.ffill(inplace=True); df_full.bfill(inplace=True); df_full.fillna(0, inplace=True)\n",
    "df_full['month'] = df_full.index.month\n",
    "df_full['day_of_week'] = df_full.index.dayofweek\n",
    "df_full['hour'] = df_full.index.hour\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "# --- 2. Feature & Target Setup ---\n",
    "selected_features = [\n",
    "    'temperature_2m', 'relativehumidity_2m', 'dewpoint_2m', 'apparent_temperature',\n",
    "    'pressure_msl', 'surface_pressure', 'vapor_pressure_deficit', 'precipitation',\n",
    "    'cloudcover', 'shortwave_radiation', 'direct_radiation', 'diffuse_radiation',\n",
    "    'windspeed_10m', 'winddirection_10m', 'windgusts_10m', 'et0_fao_evapotranspiration',\n",
    "    'soil_temperature_0cm', 'soil_moisture_0_1cm', 'visibility', 'cape',\n",
    "    'month', 'day_of_week', 'hour',\n",
    "    'pm10', 'pm2_5', 'carbon_monoxide', 'nitrogen_dioxide', 'sulphur_dioxide',\n",
    "    'ozone', 'dust', 'ammonia'\n",
    "]\n",
    "missing_cols = [col for col in selected_features if col not in df_full.columns]\n",
    "if missing_cols:\n",
    "    print(f\"!!! Warning: Missing features: {missing_cols}\")\n",
    "    # remove missing ones so selection doesn't fail\n",
    "    selected_features = [col for col in selected_features if col not in missing_cols]\n",
    "\n",
    "# Target is numeric pm2_5 24h later\n",
    "X = df_full[selected_features].copy()\n",
    "y_numeric = df_full['pm2_5'].shift(-24)  # numeric target 24h later\n",
    "model_data = X.join(y_numeric.rename('target_pm2_5_24h_later'))\n",
    "model_data.dropna(inplace=True)\n",
    "\n",
    "X_full = model_data[selected_features]\n",
    "y_full_numeric = model_data['target_pm2_5_24h_later']\n",
    "\n",
    "# --- 3. Convert numeric target to categories (for Naive Bayes classifier) ---\n",
    "print(\"\\n--- Converting Target (y) to Categories ---\")\n",
    "bins = [-np.inf, 30, 60, 90, 120, 250, np.inf]\n",
    "labels = ['1_Good', '2_Satisfactory', '3_Moderate', '4_Poor', '5_Very_Poor', '6_Severe']\n",
    "y_full_class = pd.cut(y_full_numeric, bins=bins, labels=labels)\n",
    "if y_full_class.isnull().any():\n",
    "    y_full_class = y_full_class.cat.add_categories('0_Unknown').fillna('0_Unknown')\n",
    "print(\"Target categories created.\")\n",
    "\n",
    "# --- 4. Train-Test Split (we split both numeric and class targets aligned) ---\n",
    "X_train, X_test, y_train_class, y_test_class, y_train_numeric, y_test_numeric = train_test_split(\n",
    "    X_full, y_full_class, y_full_numeric, test_size=0.2, shuffle=False\n",
    ")\n",
    "print(f\"\\nTraining set: {len(X_train)} samples\")\n",
    "\n",
    "# --- 5. Data Scaling ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Data scaling complete.\")\n",
    "\n",
    "# --- 6. Model Training (Naive Bayes classifier on CATEGORIES) ---\n",
    "print(\"\\n--- [Step 3] Training Naive Bayes Classifier ---\")\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train_scaled, y_train_class)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- 7. Build mapping: for each class, what's the mean numeric pm2_5 in training set ---\n",
    "# Ensure index alignment between y_train_class and y_train_numeric\n",
    "y_train_numeric_series = pd.Series(y_train_numeric.values, index=y_train_class.index)\n",
    "class_means = y_train_numeric_series.groupby(y_train_class).mean().to_dict()\n",
    "global_mean_numeric = y_train_numeric_series.mean()\n",
    "# Fill any missing class with global mean\n",
    "for lbl in nb_model.classes_:\n",
    "    if lbl not in class_means:\n",
    "        class_means[lbl] = global_mean_numeric\n",
    "\n",
    "# --- 8. Model Evaluation (classification) ---\n",
    "print(\"\\n--- Evaluating Naive Bayes (classification) ---\")\n",
    "y_pred_class = nb_model.predict(X_test_scaled)\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test_class, y_pred_class))\n",
    "\n",
    "print(\"\\nShowing Graph: Confusion Matrix...\")\n",
    "cm = confusion_matrix(y_test_class, y_pred_class, labels=nb_model.classes_)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=nb_model.classes_, yticklabels=nb_model.classes_)\n",
    "plt.title('Confusion Matrix - Naive Bayes')\n",
    "plt.ylabel('Actual Category')\n",
    "plt.xlabel('Predicted Category')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 9. Ask user for a FUTURE DATE and predict pm2.5 for that date ---\n",
    "print(\"\\n--- Predict pm2.5 for a future date (based on available dataset features) ---\")\n",
    "print(\"Note: model predicts pm2.5 at DATE = features_time + 24 hours.\")\n",
    "date_str = input(\"Enter the FUTURE date/time you want prediction for (e.g. '2025-11-20 15:00' or '2025-11-20'): \").strip()\n",
    "try:\n",
    "    target_date = pd.to_datetime(date_str)\n",
    "except Exception as e:\n",
    "    print(f\"Date parsing error: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "# To predict pm2_5 at target_date we need features at target_date - 24h (because model trained with shift(-24))\n",
    "features_time = target_date - pd.Timedelta(hours=24)\n",
    "print(f\"Looking for features at: {features_time} (to predict pm2.5 at {target_date})\")\n",
    "\n",
    "# Find the row in df_full closest at-or-before features_time\n",
    "if features_time in df_full.index:\n",
    "    features_row = df_full.loc[features_time, selected_features]\n",
    "    used_time = features_time\n",
    "else:\n",
    "    # get previous available timestamp (pad). If none available, abort with message.\n",
    "    idx = df_full.index.get_indexer([features_time], method='pad')[0]\n",
    "    if idx == -1:\n",
    "        print(\"Sorry — dataset mein features_time se pehle koi record nahin mila. Prediction possible nahi hai.\")\n",
    "        sys.exit()\n",
    "    used_time = df_full.index[idx]\n",
    "    print(f\"No exact features for {features_time}. Using nearest previous available timestamp: {used_time}\")\n",
    "    features_row = df_full.loc[used_time, selected_features]\n",
    "\n",
    "# If features_row is a Series (single row) keep as is; if multiple rows (possible if index not unique),\n",
    "# pick the first.\n",
    "if isinstance(features_row, pd.DataFrame):\n",
    "    features_row = features_row.iloc[0]\n",
    "\n",
    "# Prepare feature vector, scale and predict\n",
    "X_input = features_row[selected_features].values.reshape(1, -1)\n",
    "X_input_scaled = scaler.transform(X_input)\n",
    "\n",
    "pred_class = nb_model.predict(X_input_scaled)[0]\n",
    "probs = None\n",
    "if hasattr(nb_model, \"predict_proba\"):\n",
    "    probs = nb_model.predict_proba(X_input_scaled)[0]\n",
    "    # map classes to probabilities\n",
    "    class_prob_map = dict(zip(nb_model.classes_, probs))\n",
    "else:\n",
    "    # fallback: we only have predicted class\n",
    "    class_prob_map = {pred_class: 1.0}\n",
    "\n",
    "# Compute expected numeric pm2.5 as weighted average of class_means\n",
    "expected_pm25 = 0.0\n",
    "for cls_idx, cls in enumerate(nb_model.classes_):\n",
    "    prob = class_prob_map.get(cls, 0.0)\n",
    "    mean_val = class_means.get(cls, global_mean_numeric)\n",
    "    expected_pm25 += prob * mean_val\n",
    "\n",
    "print(\"\\n--- Prediction Result ---\")\n",
    "print(f\"Features used from timestamp: {used_time}\")\n",
    "print(f\"Predicted category for pm2.5 at {target_date}: {pred_class}\")\n",
    "if probs is not None:\n",
    "    print(\"Predicted class probabilities:\")\n",
    "    for cls, p in class_prob_map.items():\n",
    "        print(f\"  {cls}: {p:.4f}\")\n",
    "print(f\"Estimated numeric pm2.5 at {target_date} (prob-weighted mean): {expected_pm25:.2f} µg/m³\")\n",
    "print(\"** Note: yeh numeric estimate classifier-based approach se nikala gaya hai (weighted mean of class-means). Agar aap strictly numeric regression chahte hain toh regression model (e.g. RandomForestRegressor) recommend karunga. **\")\n",
    "\n",
    "# --- 10. Save model, scaler and metadata ---\n",
    "print(\"\\n--- Saving Model Files ---\")\n",
    "try:\n",
    "    joblib.dump(nb_model, \"nb_model.joblib\")\n",
    "    joblib.dump(scaler, \"nb_scaler.joblib\")\n",
    "    joblib.dump(selected_features, \"nb_features.joblib\")\n",
    "    joblib.dump(bins, \"nb_bins.joblib\")\n",
    "    joblib.dump(labels, \"nb_labels.joblib\")\n",
    "    joblib.dump(class_means, \"nb_class_means.joblib\")  # saved mapping for numeric estimation\n",
    "    print(\"SUCCESS: Naive Bayes model aur scaler .joblib files mein save ho gaye.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving files: {e}\")\n",
    "\n",
    "print(\"\\n--- [Step 3] Naive Bayes (TRAIN) Script Finished ---\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sys\n",
    "\n",
    "# --- 1. DATA LOAD KAREIN ---\n",
    "print(\"Data loading and model training started.\")\n",
    "try:\n",
    "    df = pd.read_csv('jhajjar_data_60_finalNEW.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'jhajjar_data_60_finalNEW.csv' file nahi mili. Check karein file wahi rakhi hai jaha code hai.\")\n",
    "    sys.exit()\n",
    "\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df = df.dropna(subset=['pm2_5'])\n",
    "\n",
    "# --- 2. CATEGORIES BANAYEIN ---\n",
    "def get_quality(pm):\n",
    "    if pm <= 30: return 'Good'\n",
    "    elif pm <= 60: return 'Satisfactory'\n",
    "    elif pm <= 90: return 'Moderate'\n",
    "    elif pm <= 120: return 'Poor'\n",
    "    elif pm <= 250: return 'Very Poor'\n",
    "    else: return 'Severe'\n",
    "\n",
    "df['pm2_5_quality'] = df['pm2_5'].apply(get_quality)\n",
    "\n",
    "# --- 3. FEATURES TAYYAR KAREIN ---\n",
    "df['hour'] = df['time'].dt.hour\n",
    "df['month'] = df['time'].dt.month\n",
    "df['dayofweek'] = df['time'].dt.dayofweek\n",
    "df['dayofyear'] = df['time'].dt.dayofyear\n",
    "\n",
    "weather_cols = [\n",
    "    'temperature_2m', 'relativehumidity_2m', 'dewpoint_2m', 'apparent_temperature',\n",
    "    'pressure_msl', 'surface_pressure', 'vapor_pressure_deficit',\n",
    "    'precipitation', 'rain', 'windspeed_10m', 'winddirection_10m'\n",
    "]\n",
    "time_cols = ['hour', 'month', 'dayofweek', 'dayofyear']\n",
    "\n",
    "X = df[weather_cols + time_cols].fillna(0)\n",
    "y = df['pm2_5_quality']\n",
    "\n",
    "# --- 4. MODEL TRAIN KAREIN ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# High accuracy ke liye C=100 use kar rahe hain\n",
    "svm_model = SVC(kernel='rbf', C=100, gamma='scale', random_state=42)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Accuracy print karein\n",
    "y_pred = svm_model.predict(X_test_scaled)\n",
    "print(f\"Model  Accuracy: {accuracy_score(y_test, y_pred):.2%}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- 5. USER INPUT (SIRF EK BAAR) ---\n",
    "user_input = input(\"Enter a future date (Format: YYYY-MM-DD HH:MM:SS): \")\n",
    "\n",
    "try:\n",
    "    target_date = pd.to_datetime(user_input)\n",
    "    \n",
    "    # Input date se features nikalein\n",
    "    hour = target_date.hour\n",
    "    month = target_date.month\n",
    "    dayofweek = target_date.dayofweek\n",
    "    dayofyear = target_date.dayofyear\n",
    "    \n",
    "    # Us mahine/ghante ka average mausam uthayein\n",
    "    mask = (df['month'] == month) & (df['hour'] == hour)\n",
    "    if mask.sum() == 0:\n",
    "        mask = (df['month'] == month)\n",
    "    \n",
    "    avg_weather = df.loc[mask, weather_cols].mean()\n",
    "    \n",
    "    # Data taiyar karein prediction ke liye\n",
    "    features = avg_weather.to_dict()\n",
    "    features.update({\n",
    "        'hour': hour, \n",
    "        'month': month, \n",
    "        'dayofweek': dayofweek, \n",
    "        'dayofyear': dayofyear\n",
    "    })\n",
    "    \n",
    "    feat_df = pd.DataFrame([features])\n",
    "    feat_df = feat_df[weather_cols + time_cols] # Column order fix karein\n",
    "    feat_scaled = scaler.transform(feat_df)\n",
    "    \n",
    "    # Predict karein\n",
    "    prediction = svm_model.predict(feat_scaled)[0]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"Date: {target_date}\")\n",
    "    print(f\"Predicted Air Quality: {prediction}\")\n",
    "    print(\"=\"*30)\n",
    "    print(\"Program Finished.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: Date format galat tha ya koi aur dikkat aayi. ({e})\")\n",
    "    print(\"Sahi format use karein: YYYY-MM-DD HH:MM:SS\")\n",
    "\n",
    "# Code yahan khatam, ab ye apne aap band ho jayega."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4773ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import sys\n",
    "\n",
    "# --- 1. Load and Preprocess Data ---\n",
    "file_path = 'jhajjar_data_60_finalNEW.csv'  # Make sure this file is in your folder\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{file_path}' not found. Please upload the file.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Convert time to datetime objects and sort\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "data = df[['time', 'pm2_5']].dropna().sort_values('time')\n",
    "\n",
    "# Use time as index\n",
    "data_indexed = data.set_index('time')\n",
    "\n",
    "# Scale the data (LSTM works best with values between 0 and 1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "values = data_indexed['pm2_5'].values.reshape(-1, 1)\n",
    "scaled_data = scaler.fit_transform(values)\n",
    "\n",
    "# Helper function to create sequences (look back X hours to predict Y)\n",
    "def create_dataset(dataset, look_back=24):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        a = dataset[i:(i + look_back), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Hyperparameters\n",
    "LOOK_BACK = 24   # Use past 24 hours to predict next hour\n",
    "EPOCHS = 30      # Increased epochs for better accuracy\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Create training sequences\n",
    "X, y = create_dataset(scaled_data, LOOK_BACK)\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# Split into Train (80%) and Test (20%) sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# --- 2. Build and Train LSTM Model ---\n",
    "print(\"Building and Training LSTM Model. Please wait...\")\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(LOOK_BACK, 1)))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model (verbose=0 hides the epoch-by-epoch loop output)\n",
    "model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0)\n",
    "print(\"Training Complete.\")\n",
    "\n",
    "# --- 3. Evaluate and Output Accuracy ---\n",
    "# Predict on test data to check accuracy\n",
    "test_predict = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Invert scaling to get actual PM2.5 values\n",
    "test_predict_inv = scaler.inverse_transform(test_predict)\n",
    "y_test_inv = scaler.inverse_transform([y_test])\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test_inv[0], test_predict_inv[:,0]))\n",
    "r2 = r2_score(y_test_inv[0], test_predict_inv[:,0])\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"MODEL ACCURACY REPORT\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"Accuracy (R2 Score): {r2 * 100:.2f}%\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 4. Prediction Logic for Future User Date ---\n",
    "def get_aqi_category(pm25):\n",
    "    if pm25 <= 30: return \"Good\"\n",
    "    elif pm25 <= 60: return \"Satisfactory\"\n",
    "    elif pm25 <= 90: return \"Moderately Polluted\"\n",
    "    elif pm25 <= 120: return \"Poor\"\n",
    "    elif pm25 <= 250: return \"Very Poor\"\n",
    "    else: return \"Severe\"\n",
    "\n",
    "# Get User Input\n",
    "last_date = data_indexed.index[-1]\n",
    "print(f\"\\nDataset ends on: {last_date}\")\n",
    "user_input = input(\"Enter the future date and time (Format: YYYY-MM-DD HH:MM:SS): \")\n",
    "\n",
    "try:\n",
    "    target_date = pd.to_datetime(user_input)\n",
    "    \n",
    "    if target_date <= last_date:\n",
    "        print(\"Error: Please enter a date strictly after the dataset end date.\")\n",
    "    else:\n",
    "        # Calculate hours difference\n",
    "        delta = target_date - last_date\n",
    "        hours_ahead = int(delta.total_seconds() / 3600)\n",
    "        \n",
    "        print(f\"Predicting for {target_date} ({hours_ahead} hours from now)...\")\n",
    "\n",
    "        # Start with the last known sequence of data\n",
    "        curr_seq = scaled_data[-LOOK_BACK:].reshape(1, LOOK_BACK, 1)\n",
    "        predicted_val_scaled = 0\n",
    "\n",
    "        # Recursive prediction loop (Output hidden as requested)\n",
    "        for _ in range(hours_ahead):\n",
    "            pred = model.predict(curr_seq, verbose=0)\n",
    "            predicted_val_scaled = pred[0][0]\n",
    "            \n",
    "            # Update sequence for next step\n",
    "            new_step = np.array([[[predicted_val_scaled]]])\n",
    "            curr_seq = np.append(curr_seq[:, 1:, :], new_step, axis=1)\n",
    "\n",
    "        # Final Result\n",
    "        final_pm25 = scaler.inverse_transform([[predicted_val_scaled]])[0][0]\n",
    "        aqi_status = get_aqi_category(final_pm25)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*30)\n",
    "        print(f\"PREDICTION RESULTS\")\n",
    "        print(\"=\"*30)\n",
    "        print(f\"Target Date : {target_date}\")\n",
    "        print(f\"Predicted PM2.5 : {final_pm25:.2f}\")\n",
    "        print(f\"AQI Category    : {aqi_status}\")\n",
    "        print(\"=\"*30)\n",
    "\n",
    "except ValueError:\n",
    "    print(\"Invalid date format. Please use YYYY-MM-DD HH:MM:SS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73729ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress TensorFlow warnings for cleaner output\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ==========================================\n",
    "# 1. Data Loading and Preprocessing\n",
    "# ==========================================\n",
    "def load_and_process_data(file_path):\n",
    "    print(f\"Loading dataset: {file_path}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: File not found. Please check the file path.\")\n",
    "        sys.exit()\n",
    "\n",
    "    # Convert time to datetime objects\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # Drop rows where target 'pm2_5' is missing and sort by time\n",
    "    df_target = df.dropna(subset=['pm2_5']).copy()\n",
    "    df_target = df_target.sort_values('time')\n",
    "\n",
    "    # --- Feature Engineering: Cyclical Time Features ---\n",
    "    # This helps the model understand daily and yearly patterns (seasonality)\n",
    "    df_target['hour'] = df_target['time'].dt.hour\n",
    "    df_target['month'] = df_target['time'].dt.month\n",
    "    \n",
    "    # Convert hour/month to sin/cos signals\n",
    "    df_target['hour_sin'] = np.sin(2 * np.pi * df_target['hour'] / 24)\n",
    "    df_target['hour_cos'] = np.cos(2 * np.pi * df_target['hour'] / 24)\n",
    "    df_target['month_sin'] = np.sin(2 * np.pi * df_target['month'] / 12)\n",
    "    df_target['month_cos'] = np.cos(2 * np.pi * df_target['month'] / 12)\n",
    "\n",
    "    return df_target\n",
    "\n",
    "# ==========================================\n",
    "# 2. Helper: Create Sequences for Time Series\n",
    "# ==========================================\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        # Predict the PM2.5 value (index 0) at the next step\n",
    "        y.append(data[i+seq_length, 0]) \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# ==========================================\n",
    "# 3. AQI Categorization Logic\n",
    "# ==========================================\n",
    "def get_aqi_category(pm25_value):\n",
    "    if pm25_value <= 60:\n",
    "        return \"Low\"\n",
    "    elif pm25_value <= 120:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "\n",
    "# ==========================================\n",
    "# Main Execution\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    FILE_PATH = 'jhajjar_data_60_finalNEW.csv'\n",
    "    SEQ_LENGTH = 24  # Look back 24 hours to predict the next hour\n",
    "    # Features: PM2.5 + 4 cyclical time features\n",
    "    FEATURE_COLS = ['pm2_5', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos']\n",
    "    \n",
    "    # 1. Load Data\n",
    "    df = load_and_process_data(FILE_PATH)\n",
    "    data_values = df[FEATURE_COLS].values\n",
    "    \n",
    "    # 2. Scale Data (Normalization)\n",
    "    # GRU models work best when data is between 0 and 1\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data_scaled = scaler.fit_transform(data_values)\n",
    "    \n",
    "    # 3. Split Data (Train 90% / Test 10%)\n",
    "    train_size = int(len(data_scaled) * 0.9)\n",
    "    train_data = data_scaled[:train_size]\n",
    "    test_data = data_scaled[train_size:]\n",
    "    \n",
    "    X_train, y_train = create_sequences(train_data, SEQ_LENGTH)\n",
    "    X_test, y_test = create_sequences(test_data, SEQ_LENGTH)\n",
    "    \n",
    "    # 4. Build and Train GRU Model\n",
    "    print(\"\\nTraining GRU Model... (This may take a few minutes)\")\n",
    "    model = Sequential()\n",
    "    model.add(GRU(64, return_sequences=True, input_shape=(SEQ_LENGTH, len(FEATURE_COLS))))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(32))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1)) # Output layer for regression\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    \n",
    "    # Training\n",
    "    model.fit(X_train, y_train, epochs=15, batch_size=64, validation_split=0.1, verbose=1)\n",
    "    \n",
    "    # 5. Evaluate and Print Accuracy\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"   MODEL ACCURACY REPORT\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred_scaled = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Inverse transform to get real PM2.5 values\n",
    "    # We need a dummy array because scaler expects 5 columns, we only have 1 (predictions)\n",
    "    def inverse_transform_col(y_scaled, scaler):\n",
    "        dummy = np.zeros((len(y_scaled), len(FEATURE_COLS)))\n",
    "        dummy[:, 0] = y_scaled.flatten()\n",
    "        return scaler.inverse_transform(dummy)[:, 0]\n",
    "        \n",
    "    y_pred_real = inverse_transform_col(y_pred_scaled, scaler)\n",
    "    y_test_real = inverse_transform_col(y_test.reshape(-1, 1), scaler)\n",
    "    \n",
    "    # Calculate Metrics\n",
    "    r2 = r2_score(y_test_real, y_pred_real)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_real, y_pred_real))\n",
    "    mae = mean_absolute_error(y_test_real, y_pred_real)\n",
    "    \n",
    "    print(f\"R2 Score       : {r2:.4f} (Closer to 1.0 is better)\")\n",
    "    print(f\"RMSE           : {rmse:.2f}\")\n",
    "    print(f\"MAE            : {mae:.2f}\")\n",
    "    print(\"=\"*40 + \"\\n\")\n",
    "\n",
    "    # 6. User Input and Future Prediction\n",
    "    print(\"Data loaded up to:\", df['time'].iloc[-1])\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"Enter a future date (YYYY-MM-DD HH:MM:SS) or 'exit': \")\n",
    "        \n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Exiting program.\")\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            target_date = pd.to_datetime(user_input)\n",
    "            last_known_time = df['time'].iloc[-1]\n",
    "            \n",
    "            if target_date <= last_known_time:\n",
    "                print(f\"Error: Date must be after the last known data point ({last_known_time}).\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Predicting AQI for {target_date}...\")\n",
    "            print(\"(Calculating step-by-step... output suppressed)\")\n",
    "            \n",
    "            # --- Prediction Loop ---\n",
    "            # Start with the very last sequence from our real data\n",
    "            current_seq = data_scaled[-SEQ_LENGTH:].copy()\n",
    "            current_time = last_known_time\n",
    "            \n",
    "            # Iterate hour by hour until we reach target date\n",
    "            while current_time < target_date:\n",
    "                current_time += datetime.timedelta(hours=1)\n",
    "                \n",
    "                # 1. Predict next PM2.5 value\n",
    "                # Reshape to (1, 24, 5)\n",
    "                input_seq = current_seq.reshape(1, SEQ_LENGTH, len(FEATURE_COLS))\n",
    "                \n",
    "                # VERBOSE=0 ensures NO output inside this loop\n",
    "                pred_scaled_val = model.predict(input_seq, verbose=0)[0][0]\n",
    "                \n",
    "                # 2. Calculate time features for this new predicted timestamp\n",
    "                h_sin = np.sin(2 * np.pi * current_time.hour / 24)\n",
    "                h_cos = np.cos(2 * np.pi * current_time.hour / 24)\n",
    "                m_sin = np.sin(2 * np.pi * current_time.month / 12)\n",
    "                m_cos = np.cos(2 * np.pi * current_time.month / 12)\n",
    "                \n",
    "                # 3. Update the sequence window\n",
    "                # Create new row: [pred_pm25, time_features...]\n",
    "                new_row = np.array([pred_scaled_val, h_sin, h_cos, m_sin, m_cos])\n",
    "                \n",
    "                # Drop oldest record, add new record\n",
    "                current_seq = np.vstack([current_seq[1:], new_row])\n",
    "            \n",
    "            # --- Final Result ---\n",
    "            # Extract the final predicted scaled PM2.5 value\n",
    "            final_pred_scaled = current_seq[-1, 0]\n",
    "            \n",
    "            # Inverse transform to get original scale\n",
    "            dummy_row = np.zeros((1, len(FEATURE_COLS)))\n",
    "            dummy_row[0, 0] = final_pred_scaled\n",
    "            final_pm25 = scaler.inverse_transform(dummy_row)[0, 0]\n",
    "            \n",
    "            # Get AQI Category\n",
    "            aqi_cat = get_aqi_category(final_pm25)\n",
    "            \n",
    "            print(\"\\n\" + \"-\"*30)\n",
    "            print(\"PREDICTION RESULT\")\n",
    "            print(\"-\" * 30)\n",
    "            print(f\"Date           : {target_date}\")\n",
    "            print(f\"Predicted PM2.5: {final_pm25:.2f}\")\n",
    "            print(f\"AQI Level      : {aqi_cat}\")\n",
    "            print(\"-\" * 30 + \"\\n\")\n",
    "            \n",
    "        except ValueError:\n",
    "            print(\"Invalid date format. Please use YYYY-MM-DD HH:MM:SS\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
